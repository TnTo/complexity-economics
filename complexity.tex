% !TeX spellcheck = en_GB
\documentclass[a4paper, headings=standardclasses]{scrartcl}

\usepackage[margin=2.5cm]{geometry}
\usepackage{authblk}
\renewcommand{\Affilfont}{\small}
\usepackage[style=apa, backend=biber, sorting=ynt, sortcites=true, useprefix=true]{biblatex}
\usepackage[autostyle=false, style=english]{csquotes}
\MakeOuterQuote{"}
\usepackage[british]{babel}
\usepackage[modulo]{lineno}
\linenumbers
\usepackage[hidelinks]{hyperref}

\usepackage{textcomp}
\usepackage{extdash}


\addbibresource{complexity.bib}

%opening
\title{On Complexity as Meta-Theory\let\thefootnote\relax\footnotetext{
		Previous versions of this paper were presented at the 24\textsuperscript{th} ESHET Summer School and at the 2023 INEM Conference. \\
		The last available version, and an italian one, is online at \url{https://github.com/TnTo/complexity-economics/}.
}}
\subtitle{a perspective from Economics}
\author{Margherita Redigonda\thanks{m@orsorosso.net - \url{https://orcid.org/0000-0003-1485-1204}}}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		The absence of a recognized definition of complexity makes very difficult to study it.
		This paper attempts to provide a definition of complexity following the insight expressed by Anderson in the 1972 article "More is Different" in an anti-reductionist perspective and recognizing the difference between the idea of complexity and the mathematical methods historically used for the study of complex systems. 
		The definition provided allows for the recognition of complexity as a meta-theory rather than as a scientific theory, describing its characteristics.
		Next, the relationship between complexity and economics is explored from a historical and methodological perspective, recognizing three different archetypes of the relationship (neo-empirical, post-neoclassical, meta-theoretical).
		Finally, what social norms hinder or would facilitate the adoption of complexity meta-theory in scientific practice are described. \\
		\textbf{Keywords:} Complexity, Meta-Theory, Reductionism\\
		\textbf{JEL Codes: B41, B59, A14}
	\end{abstract}
	
\section{On Complexity}
Dealing with the subject of complexity is generally something very difficult to do due to the ambiguous and elusive nature of the term.
First of all, complexity has now become a \textit{buzzword} that often has no meaning of its own.
In addition, there is no agreed definition of what complexity is at a scientific level\footnote{\textcite{horgan2015}, cited in \textcite{holt2011}, lists 45 different ideas which could be used as a starting point for a definition of complexity.}, making it very difficult to deal with the topic precisely.

The cause of this I believe becomes evident once one looks at the problem of defining complexity from a historical perspective. 

A list of what is undoubtedly being studied by complexity theory today has to include, among others, dynamical systems (in particular deterministic chaos and bifurcation theory) and statistical mechanics. For both disciplines of study, it is possible to trace their birth (or at least a founding moment) to the 1870s, when Poincar√© published his first work on recurrences and Boltzmann presented the first results on which statistical mechanics could be based.

On the other hand, the term complexity entered the scientific lexicon with the \citeyear{anderson1972} article by \citeauthor{anderson1972}, which opens by questioning the reductionist paradigm on which modern science is based, recognising that although one can create a hierarchy of sciences according to the reductionist view (e.g. physics \textrightarrow{} chemistry \textrightarrow{} biology \textrightarrow{} physiology \textrightarrow{} psychology \textrightarrow{} sociology) it is not possible to claim that "X is `just applied Y'".
That is, it is not possible to reformulate on the basis of the laws of the more reduced discipline all the laws of the more complex one (i.e. we cannot express, for example, all psychological knowledge by referring only to the underlying physiological phenomena).

By the time Anderson published his article, however, many mathematical methods for dealing with complex systems were already been developed, and there was already a common sense of which problems are studied by the complexity theory (as mentioned earlier, dynamic systems and thermodynamics, to which we can add, without claiming to be exhaustive, information theory, stochastic processes, computational agent models, game theory, graph or network theory).
In other words, by the time a definition appeared and an attempt was made to methodologically ground the field of research, the development of mathematical techniques to investigate it was already advanced and developed into an independent, reductionist perspective.

The tension appearing here is probably the focal point of the article: following Anderson complexity can only be anti-reductionist, but the mathematical methods of complexity theory are born and developed from a reductionist perspective.

Let us return to statistical mechanics as an archetypal example. The problem it solves is to bring a macroscopic phenomenon (the laws of thermodynamics and the irreversibility of physical processes) back to known microscopic laws (molecular dynamics) as prescribed by the reductionist paradigm.  This eliminates the need to have two different theories for the microscopic and the macroscopic, explaining macroscopic phenomena as a manifestations of microscopic laws.

This is in theory. In practice, however, macroscopic laws remain in use for reasons of simplicity and adequacy, because they better describe macroscopic reality and return representations that are simpler to use and still sufficiently accurate.
Moreover, statistical mechanics has created a new mesoscopic description where microscopic and macroscopic laws intermingle, describing phenomena that can be explained neither by one nor by the other alone, effectively requiring a new theoretical apparatus of mesoscopic laws.

In this sense, we can recover Anderson's insight: the moment one tries to reduce the macroscopic to the microscopic, one loses accuracy in describing the macroscopic (or at least practicality) while discovering a grey area that belongs to neither the macroscopic nor the microscopic and, at the same time, belongs to both.
Thus, one obtains a new formulation that is in practice unusable and unsuitable as a scientific theory for either level. Also a third intermediate level appears and it needs its own, distinct set of laws.

To proceed further in the discussion, it is probably necessary to attempt to give a definition of the object of study: complexity.

The concepts most often associated with the idea of complexity are the relationship between the parts and the whole, the difference between individual and collective behaviour, emergent properties, the interactions among elements, and non-linearity.\footnote{Non-linearity may seem like the odd card in the list, but its relationship to the other ideas is easy to show. Assume a set of elements ${x_i}$ and suppose we aggregate them by summing them as $X=\sum_i x_i$. The relation between the change of one of the elements of the set and the change of the aggregate is $\Delta X=\Delta x_i$, identifying macroscopic and microscopic changes and eliminating the need for two different representations of the dynamics. Perhaps a more common and analogous case in economics is that of log-linearity when the aggregation function is the product, i.e. $\log X = \log(\prod_i x_i)$ and $\Delta \log X=\Delta\log(x_i)$.} This is why it is common to speak of \textit{complex systems} almost as a synonym for complexity, because the word system implies not much more than a set of parts in relation to each other.

The definition of complexity I propose is the following:
\begin{quote}
A system is complex if it has to be described differently on different levels of one or more scales.
\end{quote}

This definition requires defining what a scale is and its levels are. But before doing so, I will say a few words about the idea of description.

Every scientific theory aims at an understanding of reality through a simplified representation that allows particular features of interest to be brought into focus. These representations are generally produced in the form of laws or models.
At the same time, however, each of these representations expresses only a particular description of reality that focuses on certain details while leaving out others.\footnote{One might, as a thought experiment, think that one could create a description of reality so comprehensive and precise, at once analytical and synthetic, to be able to make all others obsolete, but it would not deviate much from a 1:1 map of the world, on whose uselessness and impracticality Eco and Borges have written better than I can.}.

This observation gives us an initial insight into what complexity is: recognising that the world is made up of too many entities, link together by too many connections and relationships, to be able to isolate one at a time or study them all together with precision. Instead, it is necessary to recognise, from time to time, which are the important details in focus, the magnification necessary to see what is of interest, while being aware of what and why it is left out of the field of vision.

Following this metaphor, I try to explain what I mean by scale and levels.

Think of a microscope slide with a single sample. The sample appears
very differently depending on the magnification or focal plane chosen for observation.
These two variables, two dimensions along which to move, change the way we observe the sample and the description we can give of it.
The perception we have of the sample and thus the characteristics and properties we can
describe, vary as the two variables (magnification and focal plane) of observation vary.

So, we have introduced two dimensions, which we call scales, that present different ways, levels, of observing, or describing, the same entity.

Typical examples of scales are the geographical scale (which in reference to the economy has as levels, for example, a city, an industrial district, a country, a continent, the entire
global market) or the time scale (among whose levels we can list the short run, used for example for fixed capital models, and the long run).

Another scale, perhaps less intuitive, that finds enormous space in the description of economy is the aggregation scale, among whose levels we find the individual (micro), groups (meso) and the whole of society (macro), which allows us to describe how the concepts of heterogeneity and interaction influence the description of the economy, and thus to describe those behaviours of the individual that can only be explained by their relationship with others. 

Other dimensions along which the description varies, and which I therefore call scales here, are less intuitively scales.
For example, we can interpret the gender perspective as a scale.
The same phenomenon can be studied by ignoring the gender of the subjects involved, by using a binary perspective based on the biological sex of the subjects involved, by maintaining a still binary perspective but based on the gender and socialisation of the subjects involved, or by adopting a queer perspective by including a multiplicity of classification categories based on the self-representation of the subjects involved.
Each of these levels returns different characteristics of the studied system to the observer and none of them is a priori the correct one.
A research project in medicine on reproductive health is likely to use a perspective based on biological sex, which is, at least to a first approximation, a determining factor, without, however, introducing a greater richness of detail that would only produce noise when analysing the results statistically\footnote{At least to a first approximation. Having obtained the results of the initial study, it might be extremely useful to change the level of observation on the scale in order to be able to describe, for example, the same phenomenon in the population undergoing hormone replacement therapy following a diagnosis of gender incongruence.}.
Likewise, an ethnography on queer movements probably needs the level of detail and heterogeneity that lies in the self-representation of individuals.

To sum up, a scale is an aspect of the system, a semantic or conceptual area of it, that can be analysed from different viewpoints or with different levels of detail.
And a complex system is any system that changes its characteristics, or rather the description one can make of it, depending on the point of observation. In other words, a system that does not always remain the same and coherent in every context.

It is likely that following this definition, reality is complex and almost every subset of it is also complex. But I see no problem in this.

What this definition does, on the other hand, in addition to including -I believe- all the different insights into the nature of complexity due to the abstractness and generality of the idea of scale, is to indirectly draw attention to the approximations that are explicitly or (more often) implicitly made in any scientific research and in any description of reality.

What this definition does not do is directly provide some kind of specific knowledge in any field of knowledge.
In this sense, we cannot consider complexity theory, as it has just been defined, a theory in the strict sense.

Rather, it provides guidance on how to operationalise the observation, and thus the study, of a complex system, i.e. how to construct theories on different complex systems.

In this sense, I think it is more correct to speak of a meta-theory of complexity, i.e. a theory of how theories describing complex systems need to be developed. If what was mentioned earlier about reality itself being a complex system in all its aspects is true, then the meta-theory of complexity provides principles that any scientific theory should take into account.

It could be argued that the particular case of physics is sufficient to break down the argumentative construction I am pursuing, providing a very strong argument in favour of maintaining a reductionist paradigm.
Through logical steps it is now possible to trace almost every physical law back to a small number of fundamental forces (between one and three depending on the theory), which are thus able to explain every phenomenon of reality.

From a speculative point of view, assuming for the sake of simplicity an absolutely deterministic reality, this might indeed seem to be the realisation of the reductionist dream of tracing every phenomenon back to a handful of first principles and thus to the possibility of formulating a \textit{theory of everything} that governs reality in all its aspects. But such thinking finds no practical or experiential confirmation.
No researcher would ever try to describe even the statics of a bridge using the few fundamental laws, let alone more complex phenomena in the animal kingdom or the cultural sphere.
And as already mentioned, even in physics itself, there are phenomena that are only traced back to the unifying theory under ideal and perfect conditions, whose real inaccuracies are better explained by ad hoc theories and corrections, developed for the specific case without any claim to universality.

The anti-reductionism present in Anderson's work and in the idea of a meta-theory of complexity can then be reformulated as the rejection of the possibility of a theory of everything and a privileged point of view.

As a consequence, \textbf{knowledge cannot but be understood as contextual and functional}, i.e. linked to particular premises and purposes that highlight different characteristics of the same object of study, leading to different approximations that highlight different levels on different scales.

Doing research according to this understanding of complexity therefore requires putting the specific research question at the centre, or recognising the specific facet of reality to be observed, and taking it as a starting point.
From there, it is necessary to describe the object of study as precisely and richly as possible in order to be able to recognise as many relevant scales as possible and on each of them the most suitable levels for the purpose, i.e. to make one's premises as explicit as possible, and then to approximate the object of study to a representation of it -to a model of it- that is manageable and addressable.

The act, so central here, of describing requires careful analysis, without, at least in the first instance, shortcuts and approximations. In some contexts it is possible that this can be done with care and meticulousness using the formal language of mathematics or a language that one does not perfectly master, but in general if describing (and thus, perhaps, intimately understanding) becomes a fundamental aspect of scientific practice, a return to the use of one's mother tongue (and perhaps also the use of multi-mediality) becomes an unavoidable practice.\footnote{This is just one of the practical difficulties that the adoption of complexity meta-theory faces in contemporary research. The last section of this article seeks to explore this issue further.}.

Outlined in this way, the meta-theory of complexity resembles a methodology of the particular and the unique, which recognises that the important features of a system are multiple and different depending on the purpose, history and subjectivity of the researcher. It recognises, to some extent, the organic role of the particular within the general, rather than taking the particular as a variation on the theme of the general. 

The prominence of the particular requires extending the methods available to the researcher, and in particular recovering qualitative methods even where their use has been lost. This is because the complementary use of qualitative and quantitative methods is able to explore the same phenomenon at different levels, to obtain a richer and more multifaceted knowledge of it.

Similarly, individual experience is recognised for its own importance, at least as a determinant of the description the researcher makes (observable from the right level of the right scale), and not as a mere variation of a general archetype. 
In this sense, I think that a complexity meta-theory offers a conceptual framework for those instances that try to bring an intersectional, democratic and caring perspective into scientific practice.

To close this section, I will dwell on a lively debate in many disciplines, including economics, on the coexistence of alternative theories and the problem of choosing one theory over another. The debate on pluralism. 

The term pluralism usually indicates a view that dictates the coexistence of a plurality of alternative and, generally, irreconcilable theories within a discipline. 
This is also the case in physics where different theories for high-energy physics compete to be recognised as the 'right' theory, i.e. in agreement with all the experimental evidence.

The idea of pluralism requires the existence of a correct theory, a theory of everything that can prove alternative theories wrong.

Instead, in an approach that recognises each theory as necessarily contextual, i.e. with limits to its domain of application (as the ranges on certain scales for which it was developed), the idea of pluralism loses its meaning, as the coexistence of theories with different purposes and premises is an inherent characteristic.

Complexity meta-theory suggests to us that Newtonian physics, classical electromagnetism and even geocentrism are not wrong theories, but simply theories and models that (like all of them) have a non-universal field of validity, and that in their field of validity are useful because they answer the questions for which they were created, because they answer a purpose.

An example from economics is the study of long-term phenomena.
There are at least four different approaches in the history of economics to describe the long run: the neoclassical approach, which describes the existence of a single long-run equilibrium and the behaviour of the economy relaxing towards it; the post-Keynesian approach, which describes the possibility of multiple steady states, similar in idea to unstable equilibria, between which the economy can move; the Marxian approach, which qualitatively describes certain characteristics of the dynamics of the economy in the long run, without however describing the end point of it with sufficient precision to be studied; the deterministic chaos theory approach, which by describing the economy as a chaotic system concludes that it is impossible to study its long-term behaviour, due to the gradual accumulation of errors inevitable in any representation.
Each of these four approaches allows different aspects of the future to be studied, answering different questions.

I see no good reason for preferring a scientific paradigm that seeks to recognise which of these is the correct representation of reality in every situation, rather than reasoning about the implicit and explicit assumptions behind each of them \footnote{For example, a too often forgotten assumption of the neoclassical model is that of normal time, or the absence of exogenous shocks. I find it more interesting to discuss what makes a time \textit{normal} and so under what conditions the neoclassical representation is useful, than to stage a contest about who is \textit{absolutely} right.}.

Rather than a pluralistic view of knowledge development, complexity meta-theory takes a secular view, in which the development of each theory is a useful if not necessary building block to complete the overall picture, and in which each theory has a right in itself to exist in its own domain of application.

The intellectual clash is replaced by a dialectical approach, in which the relationship between alternative theories aims to highlight the implicit assumptions of each and their respective domains of validity, rather than the correctness of one or the other.

\section{On the Economy}
The second part of this article attempts to contextualise what has been written so far in the current state of economics, identifying in particular three archetypal relationships
between complexity and economics.
To do so, I will again start from a historical perspective.

Towards the end of the 19th century, at the same time as the mathematical methods of complexity were born, the marginalist paradigm developed in economics as a research approach inspired by reductionist principles. 

Marginalism, in fact, seeks to trace the study of economy back to the choices of ideal economic agents capable and willing to maximise specific goals. 
In particular, the problem typically studied concerns finding the conditions under which a small number of agents (usually one or two) faced with the possibility of making transactions give up because none of them is able to (marginally) improve the situation of both, a condition known as the Paretian optimum.

Starting from this apparently very simple idea, the marginalist school developed, on the one hand, mathematical methods to study this class of problems (generally using calculus on the model of Newtonian physics), and, on the other hand, an entire economic theory by deducing more properties of the system.
Marginalist theory is thus constructed by successive additions to an elementary core of assumptions and ideas.

The first archetypal situation of an economy of pure exchange (the barter) between two agents is gradually generalised to a theory of production and, much later, to a theory of the economic aggregate.

However, it would not be correct to say that the (early) marginalists did not recognise the complex nature of the economic system.
\textcite[p. 20]{marshall1988} wrote in the \textit{Principia} that "Society is something more than the sum of the lives of its individual members.", recognising the economy as a complex system.
Nevertheless, undoubtedly following the zeitgeist, the attempt being made was to explain complexity as an emergent property that can be analytically traced to a few fundamental laws, as Boltzmann did with statistical mechanics.

After a phase of relative decline in the first half of the 20th century, the marginalist approach undoubtedly became dominant in the economic discipline from the 1970s onwards. 

The pervasiveness of the marginalist school's hegemony can also be observed in how the definition of economics has changed. 
While it may seem intuitive that economics studies the economy, or that it is defined with words like 'the study of the processes of production and exchange', this is not the answer most economists would give.

In 1932 Robbins published a book in which he argued that "The [...] subject of Economic Science [is the study of] the forms assumed by human behaviour in disposing of scarce means." \parencite[p. 15]{robbins2007}.
The adoption of this definition shifts over time the unifying element of the discipline from contents to methods, allowing both a phase of imperialist expansion in the study of topics traditionally belonging to other disciplines \parencite[cf.][]{stigler1984, lazear2000} and to exclude from the positions of power and prestige those schools that do not use the tools useful to study the optimal allocation of scarce resources, or that do not formulate their research question in terms of the constrained optimisation of a certain objective function (called utility, or following Pareto ophelimity).

In practice, the marginalist idea of building up an economic theory from few assumptions about human behaviour and the choices agents face has replaced the study of the economy as the identifying element of economics.

Acknowledging the complexity of reality and taming it within the reductionist paradigm is also present in one of the articles that founded modern macroeconomics.

In 1972 Lucas wrote that "Given that the structure of an econometric model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of econometric models."\parencite{lucas1976}.
In other words, Lucas recognised that any quantitative model in economics that is based on real data has a domain of validity limited to the period of time in which the socio-cultural norms --institutions-- for which it was imagined remain in place.
In doing so, he also clearly implied the need to recognise and describe two different levels on the scale of aggregation (individual choices and the behaviour of aggregate variables) and the interactions between them. The need to integrate microeconomics and macroeconomics is made explicit.

In order to remain within the reductionist paradigm, however, the neoclassical macroeconomics that developed from Lucas's critique posed the problem of reconciling the laws of economic aggregates with fundamental microeconomic laws (i.e. microfounding macroeconomic), precluding the possibility of imagining models describing only the aggregate level that were flexible enough to accommodate changes in institutions (e.g. by recovering the methods of bifurcation theory).

The problem may seem at first sight to be the same as the one addressed by Boltzmann a century earlier and a research programme known as Econophysics has tried for years, without conclusive results, to apply the methods of statistical mechanics to the problem of microfoundations without success.\footnote{The missing element in economics to be able to draw a useful parallel with statistical mechanics is the presence of a quantity that is conserved like energy in physics. Observing some similarities this quantity should have the unit of measurement of a value, but no economic theory can provide a \textit{principle of conservation of total value} neither measured in monetary terms nor measured in terms of time (as labour value).}
The solution pursued is, at least until recently, to identify the two levels, i.e. to describe the aggregates \textit{as if} they were single agents. This approximation strategy makes it possible to obtain reductionist and analytically resolvable models, but it generates a series of paradoxes when trying to explore the links between the representative agent (of the aggregate) and the individual economic agents in reality \parencite{kirman1992}.

More or less at the same time, an exploratory workshop was organised at the Santa Fe Institute for the study of complex systems in 1987 to investigate the possible role of the emerging complexity theory in economics, with the participation of Anderson, Arrow and Arthur, among others \parencite{fontana2010a}.
Fontana reports that: "[Arrow] is not expecting the birth of an entirely new approach: the general framework should remain as it is, with the role for the `new economics', enriched by cooperation with physicists and biologists, being to improve the status quo ante." \parencite{fontana2010a} without therefore questioning the deductive-reductionist method\footnote{Adopting a deductive-reductionist approach is of vital importance for many economists because it is the method of the natural sciences (and especially of physics), which in their view allows economics to be elevated to a status of a more mature (and in some ways better as more epistemologically sound) science than the other social sciences}.

However, the direction of the economics program at the Santa Fe Institute (SFI) is assigned in the following years not to Arrow but to Arthur, who sets up the development of a new paradigm that, accepting Anderson's intuition to focus on the relationships between entities rather than seeking a unifying theory, stands as a (heterodox) alternative to the neoclassical (reductionist) paradigm.
This strand of research, mostly known as Complexity Economics or the Santa Fe Perspective, has produced numerous individual papers over the years, investigating topics and employing methods (such as computational simulations) that are generally neglected in the discipline, but without producing a unifying theory by its very nature.

In recent times, three other phenomena have affected economics, greatly influencing its relationship with complexity.

The first is the spread of computers, which has allowed for solutions obtained with computational tools to be accepted that alongside analytical solutions \parencite{cherrier2023, backhouse2016}, paving the way for macroeconomic models that partly abandon the representative agent to include a certain degree of heterogeneity (as the HANK models).

The second is the progressive shift of the discipline from purely model-theoretical research to empirical research, which bases its results on a thorough analysis of the available data \parencite{cherrier2018, backhouse2017}.
This neo-empirical turn is taken to extremes by some economists who reject the need for an economic theory (partly as a result of the limitations that emerged with the crises at the beginning of the 21st century), believing that data analysis is sufficient to reveal the cause and effect relationships underlying economic phenomena.

The third is the progressive specialisation of economists that has led to the emergence of independent research communities that nonetheless share a common origin in neoclassical theory. This phenomenon has been labelled by some as \textit{mainstream pluralism} \parencite{cedrini2018, davis2006, davis2019a} to emphasise that there is a plurality of research agendas that coexist with each other and coexist with (and indeed feed off of) the dominant (neoclassical) paradigm that preceded them.

The next sections will be devoted to describing three archetypal relationships between complexity and economics useful to reflect both on the current state of economics and on the general reception of the idea of complexity in scientific practice.
The first archetypal relation has its roots in the neo-empirical turn and makes use of the mathematical tools of complexity. The second one is the one outlined by Arrow and can be understood as one of the causes of the fragmentation described by \textit{mainstream pluralism}, which I call post-neoclassical and also makes use of the mathematical tools of complexity.
The third starts with the Santa Fe Perspective and succeeds in embracing complexity as meta-theory.

\subsection{Neo-Empirical Complexity}
The first of the three archetypal relationships is the neo-empirical one.

As mentioned, the neo-empirical strand of research in economics focuses on constructing methods that allow to find causal relationships in data, regardless of the specific research domain. These include generalisations of linear regressions (such as \textit{difference-in-differences} methods) and methods that attempt to manipulate the sample in order to obtain pairs of observations to compare (as in \textit{randomised control trials} or the use of \textit{matching} procedures).

Mathematical methods of complexity offer improvements for these two classes of methods through the \textit{machine learning} techniques developed over the years, which allow non-linear relationships to be identified and transform the representation of the sample in order to improve the procedures for matching observations.
Furthermore, complexity theory has over the years used various mathematical entities to describe the data to be analysed, while at the same time developing specific techniques for the purpose.
An example of these different representations are networks, that made it possible to analyse the data taking into account the mutual interdependence relationships of the observations. 

The "Economic Complexity" research strand \parencite{hidalgo2021} is an interesting example of how a common tool of complexity (networks) made it possible to represent an economic problem (the level of development of an economy) without significant recourse to economic theory.
The underlying assumption is that the most advanced economies produce and export complex products.
By representing global import and export flows by product class as a network, in which the nodes represent countries and the links represent commodity flows, a fixed point process can be defined to model the hypothesis.
The result of the fixed point process is an index to rank economies by their level of development.

It is fascinating that a very simple model, which uses rather simple observations as theoretical premises, succeeds through an effective representation of the data to produce results that align well with intuition.
On the other hand, the absence of a solid theoretical model makes it impossible to problematise what a developed economy is, because the definition derived from it is in a sense tautological (an advanced economy exports the products that advanced economies export), leaving no room for the analysis of historical processes (any diachronic perspective, and thus a time scale, is absent except in the comparison of indices calculated over different periods) and the institutional contexts that influence economic development.

An easy criticism of this way of introducing advanced statistical methods into the study of economics is that they are rather prone to finding a relationship in the data in every case, even when this is spurious and the effect of noise in the data. And generally speaking, data on economic processes (and social processes in general) are extremely noisy, having small numbers, measurement errors and sampling problems.

The illusion that eliminating theoretical frameworks from data analysis can produce objective knowledge runs up against the possibility of inferring spurious or non-generalisable relationships outside the analysed sample (returning to the problem of contextuality of knowledge), problems that a theoretical guide is able to reduce by directing the analysis towards the most important elements (distinguishing them from the noise) and making sense of the results obtained.

On the other hand, it is impossible to deny that a broader range of increasingly sophisticated mathematical methods has enabled and continues to enable empirical studies that were impossible with the methods historically used in economics.

\subsection{Post-Neoclassical Complexity}
The second relationship is the post-neoclassical one.

Neoclassical theory has accumulated over the years (like any long-standing scientific theory) a number of counterexamples and research questions that are difficult to address with its core set of assumptions.

To remedy this problem, the choice of some economists is not to reject neoclassical theory and replace it with one of the heterodox alternatives or the neo-empirical approach, but rather to relax some (few) assumptions in order to deal with a specific class of problems, while maintaining the same reductionist-deductive approach as much as possible.

Mathematical methods of complexity are, in some cases, the tools that allow assumptions to be relaxed while still obtaining an analytically (or computationally) tractable solution.

A very important example in the history of microeconomics is the theory of iterated games. The assumption that is relaxed is that all agents possess perfect information, allowing it to develop and improve through repetition of the game. At the same time, the time scale is introduced into the model, allowing the optimal action in the single iteration to be distinguished from the optimal action in the game as a whole.
In this way, real situations in which, for example, agents exhibit collaborative rather than competitive behaviour can be reconciled with neoclassical theory.

More recent examples include the introduction of non-linear utility functions
by behavioural economics or the use of computational agent models in innovation theory, which allow for the introduction of information asymmetries and other rigidities and the observation of how far these solutions deviate from those obtained with typical assumptions and methods.

In other cases, it is less clear which mathematical tool allows the specialisation of the theory, but it is very clear which scale is added and which assumptions relaxed. For example, environmental economics relaxes the assumption that price is an indicator of market disequilibria to try to assign a (monetary) value even to goods and externalities that are not traded in a market.
That is to say, a second notion of price is introduced that introduces a scale of the exchangeability of goods for which we must simultaneously consider the two levels of exchangeable goods (for which the typical assumptions about price formation and thus value definition apply) and non-exchangeable goods (for which a different notion of price and value must be introduced).

We can see in this archetypal relationship the same programme that drove Boltzmann: trying to develop (or use) mathematical tools that allow new phenomena to be explained with the old theory without abandoning its reductionist approach.

\subsection{Meta-theoretical complexity}
The third archetypal relationship is the one that, in fact, sets up economic research following the principles I have outlined to understand complexity as a meta-theory.

This third relation is the most difficult to focus on because, as mentioned above, it has not produced a cohesive and easily identifiable body of knowledge, but rather a collection of independent works produced by researchers who identify with the Santa Fe Perspective but do not constitute a cohesive research community.

In the spirit of the above, these works generally focus on a single research question by developing an ad hoc model, without giving rise to a stream of similar works that can mutually cite each other, creating a relational capital that allows one to become recognisable as a research niche and successfully enter the power dynamics underlying recruitment processes. An excellent overview was published by \textcite{arthur2021}.

This I believe is partly an inherent characteristic of the approach, partly a survival strategy.
Each new research question (no matter how similar to the previous ones) will correspond to new scales and hypotheses, which will necessarily produce different models and will also require time to question previously used assumptions, which does not happen when a reductionist approach is adopted.
On the other hand, as there are no major collaborations in this line of research, with the exception of the Santa Fe Institute where there are very few resident researchers anyway, the production of articles focuses on short-term projects that can be completed by a single person in time for the next position search or evaluation procedure.

Moreover, complexity economics includes a set of researchers who share a very general idea of the approach to research, whereas most other research niches identify with a single topic or method.
In this sense, I believe that the moment complexity meta-theory becomes widespread in the study of economics, it is inevitable that we will see the end of complexity economics, which would be replaced by homogeneous research programmes in terms of themes or methods working on theory development.
For the same reason, I do not believe that the complexity economy can become a cohesive and externally recognisable research community.

On the other hand, especially in recent years in which the neo-empirical turn has made the theoretical frame of reference less important, work that follows the meta-theory of complexity has sneaked in other research strands.
This is because in its secularity, complexity meta-theory is able to build and dialogue with prior knowledge, with the only caution being to use it where it is consistent in its assumptions with the assumptions made in the analysis and approximation phase of the problem.

The complexity meta-theory is necessarily heterodox not so much because it rejects the results of the neoclassical approach in their entirety, but because it rejects its search for a universal and context-independent theory.
From the point of view of complexity meta-theory, neoclassical theory is valid for the phenomena that correspond to its assumptions, while alternatives must be sought for other phenomena\footnote{The example I have in mind is oligopolistic competition between large firms that can be assumed to have optimising behaviour (profits), control all available information, and can continuously vary the number of workers and goods produced. As long as these conditions are consistent with the problem to be studied, I believe it is useful to draw on the many results that neoclassical theory has produced. On the other hand, one may question the usefulness of these results when the assumptions no longer describe the problem of interest}.

In some parts of the heterodox galaxy, I believe there is now room to introduce the idea of a meta-theory of complexity with the expectation that some of its insights will be taken up by other research traditions.
In particular, Marc Lavoie in his textbook on post-Keynesian economics \parencite{lavoie2022}, describes heterodoxies as inspired by principles of realism, holism and organicism \parencite[][p. 12]{lavoie2022}, as well as by the need for dialogue with other research traditions\footnote{Post-Keynesian economics itself can easily be described as a collection of different and complementary approaches held together by some general guiding principles that does not, however, achieve a theoretical monolithicity.}. "Realism" is the demand that the assumptions used are consistent with the reality of the objects of study, "holism" and "organicism" are the demand to consider the interactions between economic agents and the context in which they occur instead of resorting to over-stylized archetypal representations, and the need for dialogue is the recognition that each theory develops to answer certain questions while lacking the tools to investigate other classes of phenomena.
All these ideas can easily be traced back to the description I gave of a meta-theory of complexity. 

\section{On reality}
The question arises as to if doing research using the principles of a complexity meta-theory is realistic.
But this consideration can be interpreted in two very different ways: on the one hand, whether a research (eco)system can exist by embracing the meta-theory of complexity, and on the other hand, whether in the current research system it is possible for an individual researcher to work according to the principles of the meta-theory of complexity. And the answers I think are opposite.

A system that organically embraces the meta-theory of complexity is, in essence, a system that places as much if not more emphasis on setting up and developing the research (the initial phase of analysis and conscious approximation) than on the results produced. In which the preliminary work that the researcher does in describing the system to be analysed is emphasised.
Similarly, it can only be a system in which the possibility of moving between disciplines, using non-canonical methodologies or exploring new problems is not penalised by extensive and normative metrics of evaluation, in terms of number of citations or publications or membership of certain research communities that are identified with precise canons and stylemes, and, often, with lists of journals.

It is therefore necessary to make room again for work that focuses on the analysis phase and that allows the researcher to share with the tools they prefers (at the level of language, media, stylemes to be adopted, methods of communication, ...) the process of analysis and approximation, and that this is part of the evaluation of their work, an evaluation that can only be made on the very content and not through bibliometric metrics. 

This task is a form of slow work that requires researchers to have the time to carry it out without having to worry about the precariousness of their contract. On the other hand, it is a task that can be done relatively few times in one's career if, as is often the case, one concentrates on a single research question for many consecutive years.
Those who would be most affected by this change are the young precarious researchers, who already need better material working conditions.

On the other hand, I do not find the prospect of scientific research production slowing down worrying, if this is to be a consequence of more precise and careful work that moves away from the assembly-line logic that the last few years of research evaluation and allocation of funds have encouraged worldwide.

The biggest change that could be introduced, however, is on the epistemic level.
The process of analysis and approximation involves the researcher as the acting subject and is based on their own experiences and sensitivities.
The impact of gender or ethnic differences has long been ignored because they were not in the experience of researchers, particularly in economics.
The contextuality of knowledge, therefore, also derives from the subjectivity of knowledge, which is not considered in the positivist view that epistemologically populates our time.

As a consequence, part of the scientific confrontation should shift from the refutation of the results to the contextualisation of the hypotheses, opening up the problem of how, in the technical application of scientific knowledge, to converge the different readings that can be given from reality.
On the other hand, it would be easier to recognise which implicit social determinants, which unexpressed power relations, influence the creation of new knowledge and its technical application.

The answer is, therefore, that it is not a problem to adopt a meta-theory of complexity if it is also possible to change social norms and adopt a pragmatic and inductive approach to the problem of technical application, while there are numerous social norms that limit its adoption in the near future, among them the precariousness of work and the use of extensive bibliometric metrics for the evaluation of individual work.

The greatest obstacle, therefore, for the meta-theory of complexity to thrive, is the progressive corporatisation of the academia, the combination of bureaucratisation of processes and standardisation of outputs. 
This phenomenon is particularly present in economics given its highly hierarchical nature and propensity for standardisation. For example, the institutionalisation of the \textit{job market paper} as the concluding moment of the doctorate or of the strong direction that a handful of departments and journals manage to impart to the entire discipline \parencite{heckman2020, aistleitner2023}, creating an unbridgeable difference in prestige between the themes, methods and stylistic features adopted by the dominant community and all the others.

\begin{refcontext}[sorting=nyt]
	\printbibliography
\end{refcontext}

\end{document}