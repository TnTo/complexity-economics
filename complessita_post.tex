% !TeX spellcheck = it_IT
\documentclass[a4paper, headings=standardclasses]{scrartcl}

\usepackage[margin=2.5cm]{geometry}
\usepackage{authblk}
\renewcommand{\Affilfont}{\small}
\usepackage[style=apa, backend=biber, sorting=nyt, useprefix=true]{biblatex}
\usepackage[autostyle=false, style=english]{csquotes}
\MakeOuterQuote{"}
\usepackage[italian]{babel}
\usepackage[modulo]{lineno}
\linenumbers
\usepackage[hidelinks]{hyperref}

\usepackage{textcomp}

\addbibresource{complexity.bib}

%opening
\title{Sulla complessità come meta-teoria\let\thefootnote\relax\footnotetext{
		Versioni precedenti di questo lavoro sono state presentate alla 24esima ESHET Summer School e alla 2023 INEM Conference. \\
		L'ultima versione di questo lavoro è disponibile online \url{https://github.com/TnTo/complexity-economics/}.
}}
\subtitle{una discussione dalla prospettiva dell'economia}
\author{Margherita Redigonda\thanks{mciruzzi@uninsubria.it - \url{https://orcid.org/0000-0003-1485-1204}}}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		... \\
		\textbf{Keywords:} Complessità, Meta-Teoria, Riduzionismo\\
		\textbf{JEL Codes: B41, B59, A14}
	\end{abstract}
	
\section{Sulla Complessità}
Trattare il tema della complessità è generalmente qualcosa di molto difficile da fare per la natura ambigua e sfuggente del termine.
Innanzitutto, complessità è ormai diventata una \textit{buzzword} spesso priva di significato proprio.
Inoltre non esiste una definizione condivisa di cosa sia la complessità a livello scientifico\footnote{\textcite{horgan2015}, citato in \textcite{holt2011}, elenca 45 differenti idee da cui partire per cercare una definizione di complessità.} rendendo difficile trattare con precisione l'argomento.

La causa di ciò credo divenga evidente una volta che si guarda il problema della definizione di complessità da una prospettiva storica. 

Una lista di ciò che oggi è senza dubbio oggetto di studio da parte della teoria della complessità non può non contenere, tra le altre cose, i sistemi dinamici (in particolare il caos deterministico e la teoria delle biforcazioni) e la meccanica statistica. Per entrambe le discipline di studio è possibile ricondurne la nascita (o almeno un momento fondativo) agli anni 1870, quando Poincaré pubblicò i primi lavori sulle ricorrenze e Boltzmann presentò i primi risultati su cui fondare la meccanica statistica.

D'altra parte, il termine complessità entra nel lessico scientifico con l'articolo di \citeauthor{anderson1972} del \citeyear{anderson1972} \citetitle{anderson1972}, che si apre mettendo in discussione il paradigma riduzionista su cui si basa la scienza moderna, riconoscendo che sebbene si possa creare una gerarchia di scienze secondo la visione riduzionista (ad esempio fisica \textrightarrow{} chimica \textrightarrow{} biologia \textrightarrow{} fisiologia \textrightarrow{} psicologia \textrightarrow{} sociologia) non è sostenibile affermare che "X è semplicemente Y applicata".
Ovvero che non è possibile riformulare sulla base delle leggi della disciplina più ridotta tutte le leggi della disciplina più complessa (cioè non possiamo esprimere, ad esempio, tutta la conoscenza psicologica facendo riferimento solo ai fenomeni fiosiologici sottostanti).

Nel momento in cui Anderson pubblica il suo articolo, però, sono già stati sviluppati numerosi metodi matematici per gestire i "sistemi complessi" e si è già diffuso un \textit{senso comune} su quali problemi siano oggetto di studio da parte della teoria della complessità (come citato prima i sistemi dinamici e la termodinamica, a cui possiamo aggiungere, senza pretesa di esaustività, la teoria dell'informazione, i processi stocastici, i modelli computazionali ad agenti, la teoria dei giochi, la teoria dei grafi o delle reti).
In altre parole, nel momento in cui compare una definizione e un tentativo di fondare metodologicamente il campo di ricerca, lo sviluppo delle tecniche matematiche per indagarlo è già avanzato e si è sviluppato in una prospettiva indipendente e riduzionista.

Compare qui una tensione che è probabilmente il punto focale dell'articolo: seguendo Anderson la complessità non può che essere anti-riduzionista ma i metodi matematici della teoria della complessità nascono e si sviluppano in una prospettiva riduzionista.

Torniamo alla meccanica statistica come esempio archetipico. Il problema che essa risolve è di ricondurre un fenomeno macroscopico (le leggi della termodinamica e l'irreversibilità dei processi fisici) alle leggi microscopiche note (la dinamica molecolare) come prescritto dal paradigma riduzionista.  Ciò elimina la necessità di avere due differenti teorie per il microscopico e il macroscopico, riconducendo i fenomeni macroscopici a manifestazioni di leggi microscopiche.

Questo in teoria. Nella pratica, però, le leggi macroscopiche rimangono correntemente in uso per ragioni di semplicità e adeguatezza, perché descrivono meglio la realtà macroscopica e restituiscono rappresentazioni più semplici da usare e che sono comunque sufficientemente accurate.
Inoltre, la meccanica statistica ha creato una nuova descrizione mesoscopica dove le leggi microscopiche e macroscopiche si mischiano descrivendo fenomeni che non possono essere spiegati né solo dalle une né solo dalle altre, richiedendo di fatto un apparato teorico nuovo di leggi mesoscopiche.

In questo senso possiamo recuperare l'intuizione di Anderson: nel momento in cui si prova a ridurre il macroscopico al microscopico si perde in accuratezza nella descrizione del macroscopico (o almeno di praticità) mentre si esplora un'area grigia che non appartiene né al macroscopico né al microscopico e, al contempo, appartiene a entrambi.
Si ottiene quindi una nuova formulazione che è nella pratica inutilizzabile e inadeguata a essere usata come teoria scientifica per uno dei due livelli e, potenzialmente, si mette in luce un livello intermedio terzo che ha bisogno di un proprio e distinto apparato di leggi.

Per procedere oltre nella discussione è probabilmente necessario quantomeno avvicinarsi a una definizione dell'oggetto di studio: la complessità.

I concetti che più spesso vengono associati a quest'idea sono la relazione tra le parti e il tutto, la differenza tra comportamenti individuali e collettivi, le proprietà emergenti, le relazioni tra le parti, la non-linearità\footnote{La non-linearità potrebbe sembrare la carta spaiata nell'elenco, ma la sua relazione con le altre idee è semplice da mostrare. Ipotizziamo un insieme di elementi $\{x_i\}$ e supponiamo di aggregarli sommandoli come $X=\sum_i x_i$. La relazione tra il cambiamento di uno degli elementi dell'insieme e il cambiamento dell'aggregato è $\Delta X=\Delta x_i$, identificando tra loro i cambiamenti macro e microscopici ed eliminando la necessità di due rappresentazioni diverse della dinamica. Un caso forse più comune e analogo in economia è quello della log-linearità quando la funzione di aggregazione è il prodotto, ovvero $X = \log(\prod_i x_i)$ e $\Delta X=\Delta\log(x_i)$.}. Per questo motivo è comune parlare di \textit{sistemi complessi} quasi come sinonimo di complessità, perché la parola sistema sottende non molto più che un insieme di parti in relazione tra loro.

La definizione di complessità che propongo è la seguente:
\begin{quote}
	Un sistema è complesso se deve essere descritto differentemente su diversi livelli di una o più scale.
\end{quote}

Per assumere di senso, questa definizione, richiede di definire cosa siano una scala e i suoi livelli. Ma prima di fare ciò spenderò due parole sull'idea di descrizione.

Ogni teoria scientifica mira alla comprensione del reale attraverso una sua rappresentazione semplificata che permetta di metterne a fuoco particolari caratteristiche d'interesse. Queste rappresentazioni sono generalmente prodotte nella forma di leggi o modelli.
Al contempo però, ciascuna di queste rappresentazioni esprime solo una particolare descrizione del reale che si concentra su alcuni dettagli tralasciandone altri\footnote{Si potrebbe, come esperimento mentale, pensare di riuscire a creare una descrizione del reale così comprensiva e precisa, al contempo analitica e sintetica, da rendere ogni altra obsoleta, ma non si discosterebbe molto da una mappa 1:1 del mondo, sulla cui inutilità e impraticità hanno scritto, meglio di me, Eco e Borges.}.

Questa osservazione ci consegna una prima intuizione su cosa sia la complessità: riconoscere che il mondo è composto da troppi enti, legati tra loro da troppi nessi e relazioni, per poterne isolare uno alla volta o studiarli tutti insieme con precisione. Invece, è necessario riconoscere, di volta in volta, quali siano i dettagli importanti massi a fuoco, l'ingrandimento necessario per vedere ciò che interessa, consapevoli però di cosa e perché sia rimasto fuori dal campo visivo.

Seguendo questa metafora provo a spiegare cosa intendo per scala e livelli.

Pensiamo a un vetrino per il microscopio con un singolo campione. Questo campione appare
in maniera molto diversa a seconda dell'ingrandimento o del piano focale scelti per l'osservazione.
Queste due variabili, due dimensioni lungo cui muoversi, cambiano il nostro modo di osservare il campione e la descrizione che ne possiamo dare.
La percezione che abbiamo del campione e quindi le caratteristiche e le proprietà che possiamo
descrivere, variano al variare delle due variabili (l'ingrandimento e il piano focale) di osservazione.

Abbiamo cioè introdotto due dimensioni, che chiamiamo scale, che presentano differenti modi, i livelli, di osservare, o descrivere, uno stesso ente.

Esempi tipici di scale sono la scala geografica (che in riferimento al sistema economico ha come livelli, ad esempio, una città, un distretto industriale, una nazione, un continente, l'intero
mercato globale) oppure la scala temporale (tra i cui livelli possiamo elencare il breve periodo, usato ad esempio per i modelli a capitale fisso, e il lungo periodo).

Un'altra scala, forse meno intuitive, che trova enorme spazio nella descrizione dei fenomeni economici è la scala di aggregazione, tra i cui livelli troviamo l'individuo (micro), i gruppi (meso) e l'intera società (macro), che ci permette di descrivere come i concetti di molteplicità e relazione influenzano la descrizione del sistema economico, e quindi di descrivere quei comportamenti dell'individuo che trovano spiegazione solo nella sua relazione con altri. 

Altre dimensioni lungo le quali varia la descrizione, e che quindi qui chiamo scale, sono meno intuitivamente delle scale.
Ad esempio possiamo interpretare la prospettiva di genere come una scala.
Uno stesso fenomeno può essere studiato ignorando il genere dei soggetti coinvolti, utilizzando una prospettiva binaria basandosi sul sesso biologico dei soggetti coinvolti, mantenendo una prospettiva comunque binaria ma basata sul genere e sulla socializzazione dei soggetti coinvolti, o adottando una prospettiva queer includendo una molteplicità di categorie di classificazione basate sull'autorappresentazione dei soggetti coinvolti.
Ognuno di questi livelli restituisce all'osservatore differenti caratteristiche del sistema studiato e nessuno di questi è a priori quello corretto.
Uno progetto di ricerca in medicina sulla salute riproduttiva probabilmente utilizzerà una prospettiva basata sul sesso biologico che è, almeno in prima approssimazione, un fattore determinante, senza introdurre però una ricchezza maggiore di dettagli che risulterebbero solamente rumore all'atto dell'analisi statistica dei risultati\footnote{Almeno in prima approssimazione. Ottenuti i risultati dello studio iniziale potrebbe risultare estremamente utile cambiare livello di osservazione sulla scala per poter descrivere, ad esempio, gli stessi fenomeni nella popolazione sottoposta a terapia ormonale sostitutiva a seguito di una diagnosi di incongruenza di genere.}.
Allo stesso modo, un'etnografia sui movimenti queer difficilmente potrà rinunciare al livello di dettaglio ed eterogeneità che risiede nell'autorappresentazione dei singoli.

Riassumendo, una scala è un aspetto del sistema, una sua area semantica o concettuale, che possa essere analizzato da punti di vista diversi o con differenti livelli di dettaglio.
E un sistema complesso è qualunque sistema che cambi le proprie caratteristiche, o meglio la descrizione che se ne si può fare, a seconda del punto di osservazione. In altre parole, un sistema che non si mantenga sempre uguale a se stesso e coerente sotto ogni punto di vista.

È probabile che seguendo questa definizione la realtà sia complessa e sia complesso anche quasi ogni suo sottoinsieme. Ma non vedo problematicità in ciò.

Quello che fa, invece, questa definizione, oltre a includere --credo-- tutte le differenti intuizioni sulla natura della complessità grazie all'astrattezza e alla generalezza dell'idea di scala, è porre indirettamente l'attenzione sulle approssimazioni che esplicitamente o (più spesso) implicitamente vengono fatte in ogni ricerca scientifica e in ogni descrizione del reale.

Quello che non fa questa definizione è fornire direttamente un qualche tipo di conoscenza specifica in qualsivoglia ambito del sapere.
In questo senso non possiamo considerare la teoria della complessità, per come è appena stata definita, una teoria in senso stretto.

Piuttosto essa fornisce delle indicazioni su come operazionalizzare l'osservazione, e quindi lo studio, di un sistema complesso, ovvero su come costruire delle teorie sui diversi sistemi complessi.

In questo senso credo sia più corretto parlare di una meta-teoria della complessità, ovvero di una teoria su come sia necessario sviluppare le teorie che descrivono i sistemi complessi\footnote{Se è vero quanto accennato prima sull'essere la realtà stessa un sistema complesso in ogni suo aspetto, allora la meta-teoria della complessità fornisce dei principi di cui deve tenere conto ogni teoria scientifica.}.

Si potrebbe argomentare che il caso particolare della fisica sia sufficiente ad abbattere la costruzione argomentativa che sto portando avanti, fornendo un fortissimo argomento a favore del mantenimento di un paradigma riduzionistico.
Attraverso passaggi logici è possibile a oggi ricondurre quasi ogni legge fisica a un piccolo numero di forze fondamentali (tra una e tre a seconda delle teorie), che sono in grado quindi di spiegare ogni fenomeno del reale.

Da un punto di vista speculativo, assumendo per semplicità una realtà assolutamente deterministica, questo potrebbe sembrare effettivamente la realizzazione del sogno riduzionista di ricondurre ogni fenomeno a una manciata di principi primi e quindi alla possibilità di formulare una \textit{legge del tutto} che governa la realtà in ogni suo aspetto. Ma tale pensiero non trova riscontro pratico o esperienziale.
Nessun ricercatore proverebbe mai a descrivere nemmeno la statica di un ponte usando le poche leggi fondamentali, per non dire di fenomeni più complessi nel regno animale o nella sfera culturale.
E come già detto, anche nella stessa fisica esistono fenomeni che sono ricondotti alla teoria unificante solo in condizioni ideali e perfette, le cui imprecisioni reali sono meglio spiegate da teorie e correzioni ad hoc, sviluppate per il caso specifico senza pretese di universalità.

L'anti-riduzionismo presente nel lavoro di Anderson e nell'idea di una meta-teoria della complessità può essere quindi riformulato come il rifiuto della possibilità di una teoria del tutto e di un punto di vista privilegiato.

Come conseguenza, la conoscenza non può che essere intesa come contestuale e funzionale, ovvero legata a particolari premesse e scopi che evidenziano caratteristiche diverse dello stesso oggetto di studio, determinando approssimazioni diverse che evidenziano diversi livelli su diverse scale.

Fare ricerca secondo questa accezione di complessità richiede quindi di rimettere al centro la specifica domanda di ricerca, o di riconoscere la specifica sfaccettatura del reale da osservare, e assumerla come punto di partenza.
Da lì è necessario descrivere nel modo più preciso e ricco possibile l'oggetto di studio per poter riconoscere quante più scale rilevanti possibili e su ognuna di essi i livelli più adatti allo scopo, ovvero di esplicitare con la maggiore precisione possibile le proprie premesse, per poi approssimare l'oggetto di studio a una sua rappresentazione --a un suo modello-- che sia gestibile e affrontabile.

L'atto, qui così centrale, di descrivere richiede un'analisi accurata, senza, almeno in primo luogo, scorciatoie e approssimazioni. In alcuni contesti è possibile che ciò possa essere fatto con cura e minuzia usando il linguaggio formale della matematica o una lingua che non si padroneggia perfettamente, ma in generale se descrivere (e quindi, forse, comprendere intimamente) diventa un aspetto fondamentale della pratica scientifica, il ritorno all'uso del proprio linguaggio naturale (e forse anche l'uso della multi-medialità) diventa una pratica imprescindibile\footnote{Questa è solo una delle difficoltà pratiche che l'adozione della meta-teoria della complessità deve affrontare nella ricerca contemporanea. L'ultima sezione di questo articolo cerca di approfondire maggiormente la questione.}.

Così delineata, la meta-teoria della complessità si avvicina una metodologia del particolare e dell'unico, che riconosce che i tratti importanti di un sistema sono molteplici e differenti a seconda dello scopo, della storia e della soggettività di chi porta avanti la ricerca. Riconosce, in una certa misura, il ruolo organico del particolare all'interno del generale, piuttosto che assumere il particolare come variazione sul tema del generale. 

Il protagonismo del particolare richiede di estendere i metodi a disposizione del ricercatore, e in particolare di recuperare i metodi qualitativi anche dove il loro uso si è perso. Questo perché l'uso complementare di metodi qualitativi e quantitativi è in grado di esplorare lo stesso fenomeno a un maggior numero livelli diversi, per ottenerne una conoscenza più ricca e sfaccettata.

Allo stesso modo, l'esperienza umana individuale viene riconosciuta come portatrice una dimensione propria, quantomeno come determinante della descrizione che il ricercatore fa, osservabile dal giusto livello della giusta scala, e non come mera variazione di un archetipo generale. 
In questo senso, penso si possa riconoscere che la complessità così intesa offra una cornice concettuale per quelle istanze che cercano di portare una prospettiva intersezionale, democratica e di cura all'interno della pratica scientifica.

Per chiudere la sezione mi soffermo su un dibattito vivo in molte discipline, tra cui l'economia, sulla coesistenza di teorie alternative e il problema di scegliere una teoria rispetto. Il dibattito sul pluralismo. 

Col termine pluralismo si indica di solito una visione per cui all'interno di una disciplina possano (o debbano) coesistere una pluralità di teorie diverse tra loro alternative e, generalmente, inconciliabili. 
È il caso anche in fisica in cui teorie diverse per la fisica delle alte energie competono per essere riconosciute come la teoria "giusta", cioè in accordo con tutte le evidenze sperimentali.

L'idea di pluralismo richiede l'esistenza di una teoria corretta, di una teoria del tutto che possa dimostrare sbagliate le teorie alternative.

In un approccio che invece riconosce ogni teoria come necessariamente contestuale, ovvero con dei limiti al proprio dominio di applicazione (intesi come gli intervalli su alcune scale per cui è stata sviluppata) l'idea di pluralismo perde di senso, in quanto la convivenza di teorie con scopi e premesse differenti è una caratteristica intrinseca.

La meta-teoria della complessità ci suggerisce che la fisica newtoniana, l'elettromagnetismo classico e persino il geocentrismo non siano teorie sbagliate, ma sono teorie e modelli che (come tutti) hanno un campo di validità non universale, ma che nel loro campo di validità sono utili perché rispondono alle domande per cui sono state creati, perché rispondono a uno scopo.

Un esempio dall'economia può essere invece lo studio dei fenomeni di lungo periodo.
Esistono almeno quattro approcci diversi nella storia dell'economia di descrivere il lungo periodo: l'approccio neoclassico, che descrive l'esistenza di un unico equilibrio di lungo periodo e il comportamento del sistema economico che rilassa verso di esso; l'approccio post-keynesiano, che descrive la possibilità di stati stazionari multipli, simili nell'idea a equilibri instabili, fra cui il sistema economico può muoversi; l'approccio marxiano, che descrive qualitativamente alcune caratteristiche della dinamica del sistema economico nel lungo periodo, senza descrivere però il punto di arrivo di essa con precisione sufficiente a essere studiato; l'approccio che fa propria la teoria del caos deterministico, che descrivendo l'economia come un sistema caotico conclude l'impossibilità di studiarne il comportamento di lungo periodo, per il progressivo accumularsi di errori inevitabili in qualunque rappresentazione.
Ciascuno di questi quattro approcci permette di studiare aspetti diversi del futuro, rispondendo a domande diverse.

Non vedo dei motivi validi per preferire un paradigma scientifico che cerchi di riconoscere quale di questi sia la corretta rappresentazione della realtà in ogni situazione, invece che ragionare sulle ipotesi implicite e esplicite dietro ad ognuno di essi\footnote{Per esempio un'assunzione troppo spesso dimenticata del modello neoclassico è quella di tempo normale, o di assenza di shock esogeni. Trovo più interessante discutere di cosa renda un tempo \textit{normale} e quindi in quali condizioni la rappresentazione neoclassica sia utile, che inscenare una gara su chi abbia ragione \textit{in assoluto}.}.

Piuttosto che una visione pluralistica di sviluppo della conoscenza, la meta-teoria della complessità fa propria una visione laica, in cui lo sviluppo di ogni teoria è un tassello utile se non necessario per completare il quadro generale e in cui ogni teoria ha di per sé diritto a esistere nel suo dominio di applicazione.

Allo scontro intellettuale si sostituisce un approccio dialettico, in cui il rapporto fra teorie alternative mira a evidenziare le ipotesi implicite di ognuna e i rispettivi domini di validità, piuttosto che la correttezza di una o dell'altra.

\section{Sull'Economia}
La seconda parte di questo articolo cerca di contestualizzare quello che abbiamo detto
fino ad adesso all'economia, individuando in particolare tre relazioni architipiche
tra complessità ed economia. Per arrivare lì però serve un minimo di panoramica storica.
A fine 900, non del tutto casualmente allo stesso periodo in cui nascono i metodi matematici
della complessità, si sviluppa in economia il cosiddetto paradigma marginalista che è
un approccio di ricerca ispirato ai principi riduzionistici. Il paradigma marginalista
cerca di ricondurre lo studio dell'economia a le scelte di agenti economici che massimizzino
dei trade-off, delle funzioni target. E in particolare, guardando coppie o gruppi di agenti,
il problema si riduce a trovare una situazione in cui nessuna variazione, i margini, convenga
solo ad alcuni agenti e si aderettere a per altri. Il principio di ottimalità secondo pareto.
Sulla base di questa idea molto semplice che si porta dietro un preciso corredo di strumenti
matematici, tendenzialmente quelli dell'analisi matematica, la scuola marginalista costruisce
dal basso in alto, deduttivamente, un'intera teoria economica. Quindi partendo dalla descrizione
del comportamento di un agente, questi assunti di base vengono utilizzati per descrivere il
comportamento di un consumatore, di un'azienda, di un governo, per descrivere l'interazione fra
questi e via via fino ai modelli macroeconomici. La presenza di un nocciolo teorico estremamente
ben definito e limitato, sulla base del quale costruire deduttivamente il resto della teoria,
è la tipica premessa marginalista, è la tipica premessa riduzionista, in cui l'obiettivo della
teoria è quello di individuare un nocciolo piccolo e limitato da cui sia possibile dedurre tutto il
resto. Questo approccio diventa assolutamente dominante tra i anni 70-80, in cui possiamo
osservare un esempio paradigmatico di perché il marginalismo sia riduzionista. Prima di questo,
cittare o mettere forse in nota come la definizione di Robbins sposta l'attenzione
dai temi di ricerca ai metodi di ricerca, identificando quindi la disciplina col nocciolo
di assunzioni teoriche e quindi imponendo alla disciplina di basarsi su quel noccio
di assunzioni teoriche, di nuovo in una prospettiva riduzionista, in cui quel noccio di assunzioni
teoriche, di premesse teoriche, deve reggere tutta la disciplina. Nel paper del 72 di Lucas,
Lucas formula la sua famosa critica, che è essenzialmente una critica di complessità.
Nel senso che vengono riconosciuti due livelli, le scelte individuali e la dinamica degli aggregati,
viene riconosciuta la necessità di dei meccanismi di feedback fra questi due livelli,
di feedback complessi che cambino sostanzialmente la relazione fra le due cose.
E quindi in qualche modo di creare un collegamento fra macroeconomia e microeconomia.
Volendo però rimanere nell'approccio marginalista, si sollevano due problemi. Il primo è che è
necessario identificare uno dei due livelli con l'altro, invece che creare una dinamica meso che
faccia da ponte fra le due cose. Ed essendo riduzionista, questo livello non può che
essere quello microscopico, quello microeconomico, dove si applicano gli assomi dei fondamenti
teorici della teoria a cui si fa riferimento.
Mettere in nota brevemente che il problema delle microfondazioni è un falso problema,
nel senso che se posiziamo una dinamica degli aggregati flessibile abbastanza e soprattutto
non lineare abbastanza, non è impossibile riconoscere la critica di Lucas, cioè avere
dei modelli che cambiano qualitativamente il proprio comportamento al cambiare di certi
parametri, senza dover per forza esplicitamente inserire una descrizione del comportamento
individuale. Un luogo dove cercare per ciò è per esempio la teoria delle bifurcazioni.
Tra le assunzioni nel nocciolo fondamentale della teoria neoclassica c'è quella della
possibilità analitica dei problemi e della ricerca di soluzioni analitiche. Cita Serrier.
Come conseguenza una vera microfondazione all'aggregato sul modello delle macchinacce
statistica di Boltzmann è preclusa all'economia. Nota, il tassello mancante per applicare la
meccanica statistica all'economia è l'esistenza di una quantità conservata, nel senso che
il valore dei beni in economia non si conserva, ma può variare al cambiamento dei prezzi,
e non solo delle quantità, e non c'è altro candidato che permetta di mettere insieme
diverse di attività economica. Come ci insegna il problema di misurare attività economica
che viene fatta tendenzialmente utilizzando il GDP, perché la conversione in valore è
lo strumento sviluppato in economia in valore monetario, per comparare cose diverse. Ciò
apre, ma non è intenzione mia di scuterlo qua, il problema di cosa sia il valore e come
misurarlo correttamente. Quindi in assenza degli strumenti analitici o meglio delle ipotesi
sufficientemente restrittive per rendere il modello ben educato rispetto alla necessità
di una microfondazione esplicita sul modello delle macchiniche statistiche di Boltzmann,
la strada percorsa da Lucas è quella dell'identificazione dei due livelli, cioè di descrivere il macro
come micro, in qualche modo ignorando la sua stessa intuizione di dualità tra macroscopico
e microscopico. Probabilmente in nota, la giustificazione di ciò a livello retorico
di approssimazione del tutto come una sua parte, non trova giustificazione matematica
come mostrato in Kierman 92. Ed è nel periodo dell'affermarsi della teoria neoclassica
erede della teoria, ed è nel periodo dell'affermarsi gli sviluppi della teoria marginalista come
teoria hegemonica in economia, che al Santa Fe Institute si comincia a ragionare di come
integrare questo nuovo paradigma nello studio dell'economia, il paradigma della complessità.
I primi workshop esplorativi vedo una partecipazione tra gli altri di Anderson, citato all'inizio
del paper, Arrow e Brian Arthur. Fontana 2010 riporta che Arrow recupera la citazione
dalle slide di Ancona, in qualche modo mantenendo la prospettiva riduzionista e sognando di riuscire
a realizzare quanto fatto da Boltzmann con la meccanica statistica. Cioè di usare
complessità per unire e migliorare teorie, senza perdere però l'approccio deduttivo-riduzionista.
L'importanza dato dal marginalismo in poi al mantenere un approccio deduttivo e riduzionista
ha probabilmente a che fare col tentativo di distaccare l'economia dalle altre scienze
sociali e avvicinarla alle scienze naturali, in particolare alla fisica, adottando nei metodi.
D'altra parte, però, la direzione del Cento per l'economia passa rapidamente a Brian Arthur,
che invece insedia a Santa Fe un programma diverso, che cerca effettivamente, seguendo
l'intuizione di Anderson, di utilizzare la complessità in senso epistemico, avvicinandosi
molto a quello che prima abbiamo scritto come meta-teoria, sviluppando in qualche modo una
scuola di pensiero originale e in necessaria contrapposizione all'approccio neoclassico-riduzionista.
Cita Brian21 e l'Altro Fontana 2010 dopo la prossima frase.
L'alternatività del paradigma sviluppato da Arthur, noto come economia della complessità
o prospettiva di Santa Fe. La necessità dell'alternatività di quest'approccio a quello neoclassico risiede
esattamente in quanto stiamo dicendo all'inizio dell'articolo, cioè che, seguendo l'intuizione
di Anderson, la complessità non può che essere non riduzionista, mentre la teoria neoclassica è essenzialmente riduzionista.
In qualche modo l'approccio di Arthur ritorna a uno stile più classico di fare economia,
in cui i problemi sono trattati un alla volta ed eventualmente messi a sistema, senza la
necessità di sviluppare una teoria del tutto, a priori. E così facendo si riescono a reintrodurre
una serie di scale di livelli su questi che invece sono essenzialmente trascurati nella
teoria marginalista, perché è incapace di essere descritti effettivamente del suo nucleo di assioni,
tra cui quello della trattenibilità analitica. Nello stesso periodo, agli anni 80-900, con l'arrivo
dell'automazione dei calcolatori, si fa spazio nell'economia la possibilità di risolvere problemi
computazionalmente, e di, come conseguenza, progressivamente rilassare almeno in parte
le premesse, la richiesta di analiticità delle soluzioni. L'approccio computazionale è fin da subito molto presente
nell'economia della complessità, mentre la sua relazione con l'economia neoclassica è più lenta e meno lineare.
Citaro un po' di Roba de Scherrier.
Questi tre blocchi costitutivi, la teoria neoclassica, la prospettiva di Santa Fe e l'adozione di tecniche
computazionali, sono i tre blocchi su cui ancora oggi, quaranta anni dopo, possiamo provare a descrivere
la relazione tra complessità ed economia.
Individuo appunto tre relazioni archietipiche, ovvero tre modi in cui la complessità è entrata a far parte
della disciplina economica. Chiaramente sono archietipi, quindi esistono dei lavori che si pongono in posizioni
intermedia tra due, o forse anche tre di essi, ma ci servono per orientarci, per orientare il discorso
e per provare a descrivere in maniera efficace lo stato oggi della disciplina.
La prima relazione archietipica è quella che delineava Harrow, e che mi piace definire post-neoclassica, prima di questo.
Tre tasselli su cui costruire.
L'ultimo elemento di contesto che voglio sottolineare, che voglio mettere sul tavolo, è la descrizione che Davis ed altri
fanno dello sviluppo dell'economia, particolarmente negli ultimi vent'anni, particolarmente dopo la grande crisi finanziaria.
In cui riconoscono una nota.
Aggiungere prima un discorso sulla parte del pluralismo di città, sul fatto che è accettabile trovare esperienze diverse
per uno stesso fenomeno in una scienza non sperimentale, e che in realtà anche gli sperimenti sono delle scelte,
probabilmente in una nota pie di pagina.
Davis ed altri riconoscono un patent di specializzazione negli sviluppi dell'economia moderna.
In cui ha assunto un cuore neoclassico, che non viene sostanzialmente messo in discussione, ma che dall'altra parte
ha in qualche modo finito la sua capacità propria di sviluppare una nuova teoria, attorno a questo cuore neoclassico
Adesso si sviluppano filoni di ricerca differenti, che rilassano o modificano alcune ipotesi sul comportamento degli agenti
In cui riconoscono un discorso sulla trattabilità analitica, creando delle branche di specializzazione
che sebbene condividano un'origine comune, si sono distanziate al punto da essere di fatto non comunicanti tra loro
E quindi rendendo difficile, a differenza di quanto succedeva il passato, la partecipazione di uno studioso al dibattito
in più di uno di questi filoni di ricerca
Questo fenomeno loro lo definiscono mainstream pluralism
Per evidenziare che sono comparse una pluralità di teorie su problemi specifici dell'economia
ma che in qualche modo rimangono tutte all'interno di una grande famiglia di impostazione neoclassica che ancora costituisce il mainstream della disciplina
Aggiungere una nota più di pagina con un esempio di alcune di queste sottobranche
con alcuni esempi, per esempio l'economia ambientale con il problema di prezzare cose non prezzate
E quindi di inferire dei prezzi in assenza di mercato
L'economia comportamentale che assume la possibilità di funzioni di utilità meno lisce o con termini di interazioni, eccetera, eccetera
E poi pensaci un po' nei prossimi giorni
L'altro elemento di contesto che serve è, più o meno nello stesso periodo, la progressiva empirizzazione della disciplina economica
Quindi un passaggio da una presenza molto forte della teoria
ha un approccio che invece si basa molto di più su un'analisi quantitativa dei dati
in cui la teoria gioca un ruolo marginale, per esempio nell'ispirare il tipo di metodi usati
nota sulla relazione tra regressioni lineari e teoria neoclassica, essendo che entrambi si concentrano sui margini
o addirittura, escludendo del tutto la teoria, come in approcci più moderni basati sul machine learning
o su altre tecniche statistiche avanzate
Questa empirizzazione della disciplina può essere vista come una risposta alle crisi d'inizio secolo
per cui la teoria neoclassica non ha proposto dei grandi framework concettuali per supportarle
La maggiore disponibilità di computer e strumenti di calcolo
Il rapporto tra mainstream pluralism e empirizzazione non è necessario
ma vale la pena far notare che molti degli approcci riconosciuti nel mainstream pluralism
in qualche modo superano la teoria neoclassica per spiegare meglio i fenomeni empirici
e fanno proprie tecniche sperimentali e altre tecniche di analisi dati
in qualche modo contribuendo a un progressivo spostamento della disciplina su basi meno teoriche e più empiriche
I tre architipi li registri un'altra volta

In tutto ciò non sarebbe però corretto, al di là dell'anacronismo, affermare che i (primi) marginalisti non riconoscessero la natura complessa del sistema economico.
\textcite[p. 20]{marshall1988} nei \textit{Principia} scrive che "La società è qualcosa in più della somma delle vite dei singoli"\footnote{"Society is something more than the sum of the lives of its individual members." (traduzione mia).}, richiamando di fatto il concetto di emergenza che è uno dei pilastri dello studio dei sistemi complessi.


La distanza però tra parole e pratiche ritorna anni più tardi in un momento fondamentale dello studio della macroeconomia quando la critica di Lucas \parencite{lucas1976} porta all'introduzione del concetto di microfondazione e all'abbandono nella tradizione neoclassica dei modelli macroeconomici aggregati di stampo keynesiano.

Lucas scrisse che "Dato che la struttura di un modello econometrico si basa sulle regole degli agenti per ottenere una decisione ottimale, e che queste regole variano sistematicamente quando occorrono dei cambiamenti nella struttura delle serie rilevanti per colui che deve prendere la decisione, allora ogni cambiamento nelle politiche altererà sistematicamente la struttura del modello econometrico"\footnote{"Given that the structure of an econometric model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of econometric models." \parencite{lucas1976} (traduzione mia).}.

\subsection{Complessità neo-empirica}
La prima delle tre relazioni archietipiche è quella che possiamo definire neoempirica.
Si inserisce appunto in un filone di ricerca che si muove progressivamente da un approccio
principalmente teorico, un approccio principalmente applicato di creazione di conoscenza
con un'enorme scusione sulla generalizzabilità di questa conoscenza che si potrebbe fare
attraverso la ricerca di relazioni causali di pattern nei dati.
Nella sua versione più semplice, la svolta neoempirica in economia si manifesta con tutta una serie di studi
che si allontanano spesso dai temi classici dell'economia
e che possiamo raggruppare per le scelte tecniche che fanno, per gli strumenti che usano.
Tipicamente evoluzioni del modello di regrezione lineare come gli studi difference in difference,
le procedure di matching, i randomized control trials e i quasi esperimenti,
ovvero un insieme di tecniche che cercano da un insieme di dati che possono essere divisi in due o più gruppi
che differiscono per qualche caratteristica, cercare di recuperare le determinanti di questa caratteristica che distingue i gruppi.
Chiaramente questo tipo di analisi ha innanzitutto un grosso limite, cioè che tende a concentrarsi su differenze al primo ordine
e su problemi che possono in qualche modo essere delineati in un senso di relazione causale,
di confronto fra sottogruppi.
E' velocemente evidente che la possibilità di usare rappresentazioni più complesse o metodi di analisi più complessi
come ad esempio la teoria delle reti o numerosi algoritmi di machine learning permetta in qualche modo di ampliare il tipo di analisi che si può svolgere.
Un esempio architipico è il lavoro di Aidalgo sull'indice di complessità economica,
in cui l'unica assunzione teorica che viene svolta è che l'export di un paese sia il rappresentativo della produzione di quel paese
e che in un approccio puramente empirico basato sull'analisi delle reti di commercio tra i paesi
va a definire un indice di complessità di in qualche modo il livello di avanzamento delle economie mondiali,
lavorando su quella che può sembrare una tutologia, cioè l'assunto è che paesi ad economia avanzata producono beni avanzati
e beni avanzati vengono prodotti in paesi ad economia avanzata.
Questa è tutta la teoria economica che c'è dietro ai lavori di Aidalgo,
che possiamo dire che da un certo punto di vista è geniale, nel senso che con una descrizione tutto sommato semplice,
stilizzata e un buon uso dei dati, riesce a ottenere dei risultati che in qualche modo si allino nell'intuizione.
Ma ci rende estremamente difficile problematizzare che cosa sia un'economia avanzata, perché un'economia avanzata sia tale
e quali siano i contesti istituzionali, i percorsi storici e le relazioni di potere che ci portano a riconoscere come avanzata
un certo tipo di economia e riconoscere quello che ci aspettiamo da un'economia avanzata in certi paesi e non in altri.
Stupidamente manca una prospettiva temporale in questo approccio e impossibile da includere se non forzatamente,
cioè facendo una sequenza di studi in un preciso posto nel tempo, ma senza la possibilità di inserire degli effetti di lungo periodo,
che per esempio impedisce di introdurre una prospettiva colonialista nel discorso.
Dall'altra parte l'uso di metodi statistici sempre più avanzati come il machine learning, come le reti neurali per classificare
o altri strumenti del genere, rende sempre più difficile distinguere il rumore dall'effetto.
I modelli machine learning sono estremamente pronti all'overfitting, ovvero a riconoscere dei pattern anche nel rumore.
E considerando che la qualità dei dati economici è di solito molto bassa e sono generalmente molto rumorosi,
il fatto di muovere verso algoritmi più complessi per cercare di sfuggire da una teoria che appare probabilmente perché è parziale e politica,
rischia di produrre una cattiva scienza, nel senso che ci priva della capacità di leggere quei dati in una maniera che permetta di evidenziare
quali siano gli elementi davvero importanti di quei dati e quali gli elementi trascurabili. Tutto ciò una macchina non può saperlo a priori.
E se i dati sono molto rumorosi è anche difficile inferirlo. Sono cose che noi ricercatori possiamo fare conoscendo il contesto in cui i dati sono presi,
conoscendo il fenomeno che vogliamo studiare e riuscendo a fare una serie di assunzioni teoriche che ci permettono di distinguere
quali sono gli elementi importanti e quindi le scale da tenere in considerazione e quali elementi è probabile che diano correlazioni spurie
o siano solo portatori di rumore perché non c'è motivo concettuale teorico per cui debbano entrare prepotentemente nella relazione che stiamo cercando di studiare.

\subsection{Complessità post-neoclassica}
La seconda relazione è quella che, invece, amo chiamare post-neoclassica e che per tanti
aspetti si avvicina all'idea di mainstream pluralism di Davis et alta.
Il tipo di riflessione che da qui nasce è che la teoria mainstream ha mostrato negli ultimi vent'anni una serie di limiti.
Nel suo essere, probabilmente perché riduzionista, eccessivamente rigida per trattare tutta una serie di situazioni reali che, per qualche motivo, sono diventati maggiori interesse, per esempio la questione ambientale o la fragilità finanziaria.
L'approccio degli studiosi post-neoclassici, che non si riconoscono generalmente come post-neoclassici, è una distinzione che introduco io in questo momento,
non è di rifiutare la teoria cercando rifugio in relazioni empiriche che possano essere estratte da dei dati, facendo un passo indietro e nascondendo le proprie intuizioni teoriche dietro delle procedure standardizzate.
Ma è piuttosto quello di cercare di riflettere su quali assunzioni del proprio modello teorico creano questa rigidità e trovare dei modi tendenzialmente ancora deduttivi di aggirarle.
In questo senso i metodi di matematica e complessità forniscono una serie di strumenti interessanti per, appunto, introdurre delle ipotesi meno restrittive,
ma che rimangano in qualche modo risolvibili, perché portano la formalizzazione del sistema verso una di quelle categorie di sistemi che sono studiate nella teoria della complessità.
Questo tipo di passaggio ha in realtà un esempio storico molto più antico della crisi del mainstream, che è l'adozione in microeconomia della teoria dei giochi.
Nel momento in cui si adotta lo strumento della teoria dei giochi, e particolarmente dei giochi terrati, che sono dei sistemi complessi,
nel senso che, per mantenere il lessico che abbiamo introdotto, introduce una scala di temporalità nell'iterazione del gioco,
è una scala di interazione in cui l'individuo non è più solo, ma interagisce con altro,
e quindi si crea una distinzione tra la descrizione del comportamento dell'individuo e la descrizione della coppia o del gruppo di persone che interagiscono nel gioco.
L'introduzione a teoria dei giochi, in particolare dei giochi iterati, appunto, permette alla microeconomia neoclassica di ampliare il proprio ambito di studi,
riuscendo a alleviare alcune ipotesi, per esempio quella di informazione perfetta, che emerge l'informazione nell'iterazione del gioco.
E quindi diventa poi interessante andare a vedere qual è la convergenza della strategia,
oppure nella simmetria delle funzioni di utilità, permettendo di avere funzioni di utilità simmetriche,
vedendo come queste interagiscono verso un equilibrio piuttosto che un altro.
E quindi questo approccio, in sintesi,
non mira a scartare, come nell'approccio noempirico, o sostituire, come vedremo poi in un'altra prospettiva, la teoria neoclassica.
Ma ha in una modalità riduzionista alla Boltzmann di dotarsi degli strumenti analitici o più recentemente computazionali
che permettano di mantenere il nocciolo teorico di riferimento, l'approccio epistemico di riferimento,
ma definire dei casi particolari in cui alcune ipotesi vengono rilassate,
riconducendo il caso base a un caso particolare della nuova trattazione.
Esempi più recenti possono essere l'introduzione di funzioni di utilità non lineari
in una parte dell'economia comportamentale esperimentale
L'utilizzo di modelli ad agenti computazionali in alcuni ambiti di teoria dell'innovazione,
o in generale di microeconomia, in cui rinunciando alla trattabilità analitica
e accettando una trattabilità computazionale
si possono introdurre assimetrie informative, rigidità
o funzioni di utilità non standard
e osservare l'interazione tra gli agenti quanto e come si discosta
dagli equilibri ipotizzati sotto le ipotesi più restrittive standard.
In maniera meno evidente,
perché non è così chiaro quali sono gli strumenti adottati,
ma diventa evidente se lo guardiamo da un punto di vista delle scale,
rientra senza dubbio in questo filone anche l'economia ambientale.
Che si pone il problema di definire un prezzo per beni non di mercato.
E quindi deve trovare una definizione di prezzo diversa da quella standard
che si applichi anche a una definizione di prezzo diversa da quella standard
che si applichi anche a beni di per sé non di mercato.

\subsection{Complessità meta-teorica}
La terza relazione archietipica è quella che utilizza la complessità come metatheoria
e si sviluppa a partire dai lavori di Brian Arthur e il resto del gruppo a Santa Fe, spesso chiamata economia della complessità.
Questo terzo archietipo è anche il più difficile a descrivere perché non ha lasciato dietro
di sé un gran numero di lavori o una scuola ben definita, in parte per le sue naturali
caratteristiche, in parte perché il modo di fare ricerca seguendo l'idea di una complessità
come metatheoria si sposa male con le norme sociali dell'accademia contemporanea,
in particolare dell'economia. Forse mettere una nota sul grado di gerarchizzazione della disciplina.
Questo perché i lavori sviluppati in questo campo sono per lo più autoconsistenti,
nel senso che partono da una domanda di ricerca, lavorano per sviluppare generalmente un modello
che risponde a questa domanda e poi finiscono lì, senza generare un flusso continuo di pubblicazioni
su argomenti simili. Questo credo che sia abbastanza conseguenza di un approccio olistico
piuttosto che un approccio riduzionistico. Questo perché se noi partiamo da dimensione
olistica della disciplina, l'approccio sarà quello di partire dal tutto, la realtà,
il mondo o qualche altra forma di totale, e poi procedere per sottrazione eliminando
quelle scale e quei livelli sulle scale per cui il nostro fenomeno di interesse,
il fenomeno che risponde alla nostra domanda di ricerca, non si manifesta o si manifesta
in maniera costante e quindi risultano non di interesse. Questo fa sì però che cambiando
la domanda di ricerca, questo processo di analisi e di approssimazioni successive debba
essere ripetuto, ogni volta per ogni nuova domanda di ricerca, portando potenzialmente
a scartare delle scale o a spostarsi su dei livelli che non erano stati prese in considerazione
prima. Come conseguenze di ciò, le ipotesi di lavoro tendono a essere diverse studio
per studio, al contrario di un approccio riduzionista in cui le ipotesi di lavoro sono
le stesse, il punto di partenza è lo stesso e quindi in qualche modo il processo di approssimazioni
successive non è ripetuto ogni studio ma è svolto una volta all'inizio del fenomeno
di ricerca e poi più o meno non ripetuto, a cui poi a questo corpo di ipotesi di lavoro
vengono fatte delle piccole aggiunte, delle piccole modifiche, in questo senso si mantiene
comunque un discorso di approssimazioni successive che permettano di rispondere a nuova domanda
di ricerca in una maniera leggermente diversa rispetto alla vecchia domanda di ricerca.
Viene da sé che questo approccio ha due grossi limiti. Il primo è quello di introdurre una
fase di ricerca assente nell'approccio classico riduzionistico che è quella dell'adeguamento
e della ridefinizione delle ipotesi di lavoro in base alla specifica domanda di ricerca,
allungando quindi tempi per la produzione di prodotti della ricerca e quindi di materiale
rendi contabile nelle procedure amministrative di carriera proprie dell'accademia neoliberale.
Dall'altra parte rende più difficile inserirsi in un filone di letteratura che ti riconosca
come parte organica di esso e che quindi ti permetta di entrare in un naturale gruppo di
autori che tra di loro si citano perché riconoscono lavori affini, lavori simili nel senso di variazioni
sul tema di uno stesso nocciolo iniziale da cui poter attingere dettagli, idee, soluzioni
senza però mettere in discussione tutto l'approccio ed entrando in un gruppo di ricercatori che
si identificano si entra in un gruppo di ricercatori che si citano migliorando le metriche che vengono
utilizzate per i processi valutativi dell'accademia neoliberale. In sintesi quindi
questo terzo archetipo non è riconoscibile sulla base di un particolare uso degli studenti
matematici della complessità o di particolare ipotesi di lavoro ma da un certo modo di fare
scienza, da un certo modo di costruire le ipotesi di lavoro con cui lo studio viene poi eseguito.
Altro dettaglio che rende in parte più difficile creare un'identità e che spiega dall'altra parte
molto bene in che senso la teoria della complessità in senso metateorico non è di per sé una teoria
perché non produce un corpo coerente, almeno non in prima battuta, di metodi pratiche,
conoscenze e ipotesi condivise, ma esplica un metodo di lavoro.
Un metodo di lavoro che si pone in relazione essenzialmente dialettica e inclusiva rispetto
alla teoria già sviluppata da precedenti teorie. La necessità per un approccio complesso
di rifiutare la teoria neoclassica dominante non è strettamente sulla scelta delle ipotesi,
ma sull'assolutizzazione delle ipotesi, sul non riconoscere, cioè che le ipotesi di lavoro
della teoria neoclassica non sono assolute e valide per la descrizione di qualunque fenomeno
economico, ma che debbano e possano essere usate solo per quelle domande di ricerca che,
al termine del processo di analisi e approssimazioni successive,
delinino un fenomeno da studiare che sia coerente con le ipotesi neoclassiche.
Si può pensare, per esempio, che alcuni mercati finanziari o mercati costellati da
un certo numero di grandi aziende rispecchino in fin dei conti le assunzioni neoclassiche di
un livello di conoscenza magari non perfetta ma comune fra i vari agenti, un comportamento
ottimizzante dei vari agenti, la presenza di variabile continua nel determinare i comportamenti
dei vari agenti e che quindi la descrizione neoclassica dell'oligopolio o del mercato o
del monopolio possa per questo tipo di aziende essere una buona descrizione per alcuni comportamenti
di queste aziende, ma non perché le ipotesi neoclassiche siano a priori corrette, ma perché
nell'analizzare il fenomeno riconosciamo che le approssimazioni che portano alle ipotesi
neoclassiche sono, per la sensibilità dello studioso, coerenti con il fenomeno oggetto di studi.
E anche in questo senso, come scrivevo prima, non ritengo corretto vedere l'approccio metateorico
alla complessità come qualcosa di pluralistico, nel senso che nel pluralismo l'idea è che
differenti teorie competano o che esistino differenti teorie, mentre il punto di vista
che cerco di portare è che queste teorie, queste che oggi chiamiamo teorie, sono in
realtà aspetti complementari di un'osservazione unificata, che non dobbiamo contrapporre
una teoria poschinesiana e una teoria neochinesiana, dobbiamo riconoscere quali sono le ipotesi
di lavoro sottostanti a queste due teorie e quindi essere in grado di riconoscere che
una teoria economica ampia sia in grado di attingere da entrambe queste tradizioni,
da entrambe queste fonti di conoscenza a seconda dello specifico fenomeno di interesse.
Questa cosa la metto un po' così, forse andrà in nota, forse nel testo.
Va la pena notare che Marc Labois, un importante esponente della scuola poschinesiana contemporanea,
nel suo libro di testo descrive le caratteristiche che un'etero-dossia e in particolare scrive
l'etero-dossia poschinesiana dovrebbe avere. Fra queste ci sono l'olismo, il realismo,
la necessità di dialogare con altre discipline, altre scuole per superare i propri limiti.
Mi sembra interessante evidenziare questo perché già si potrebbe discutere che il
noccio di assunzioni fondamentali dell'economia poschinesiana sia tutto sommato limitato essendo
questa bene o male un contenitore al suo interno piuttosto eterogeneo.
Dall'altra parte sembra accennare una possibile convergenza,
fra quanto discusso in questo paper e le pratiche di ricerca con la comunità.
E' sicuramente un discorso da approfondire maggiormente, soprattutto nel cercare di
individuare quale siano effettivamente le ipotesi in uso da parte di alcuni o tutti
i tipi poschinesiani, che siano assunte come ipotesi di lavoro standard non giustificate
da un processo di analisi e di approssimazione della realtà in base agli argomenti di studio
tipici affrontati dalla scuola.

\section{Sulla realtà}
I want to empathize that a sincerely complex approach puts at leat as much emphasis on the processes than on the results, if not more. How a research question is chosen and how a model is tailored (and so the reasoning behind it) are a fundamental part of how research is doing and should be communicated and valorized on its own, where results remain a useful appendix of doing science.

È lecito chiedersi quanto sia realistico il programma di ricerca che sto proponendo,
il metodo di ricerca che sto proponendo.
Dividerei questa riflessione sul realismo in due parti.
Una prima scevra da norme sociali e limiti amministrativi e una seconda invece calata
nel contesto contemporaneo di aziendalizzazione dell'università.
Di per sé non è un programma di ricerca che fondamentalmente mini l'attuale modo
di far ricerca.
Ricchiede da parte del ricercatore una maggiore consapevolezza, soprattutto gli studi iniziali
della ricerca, per poter riconoscere quali sono le ipotesi di lavoro che sta effettivamente
utilizzando, al fine di poter discutere criticamente la loro relazione con l'oggetto di studi,
con i fenomeni che si stanno studiando.
Non mi è difficile immaginare che un approccio del genere possa portarci a dover sviluppare
un'effettiva rivoluzione epistemica, perché il processo di approssimazione è almeno
in parte legato alla sensibilità del ricercatore e quindi si introduce una dimensione suggestiva
non tanto nel processo deduttivo in sé che ci porta a definire le ipotesi di lavoro,
ma nella scelta, nella possibilità di prendere per vere certe approssimazioni, anche solo
perché necessariamente un primo studio che voglia arrivare a definire delle proprietà
di carattere generale dovrà tenere conto solo delle cause o delle relazioni maggiormente
significative e quindi approssimando quelle che potremmo chiamare relazioni o correlazioni
fenomeni di secondo ordine e distinguere cosa sia una approssimazione, cosa sia una relazione
di primo o di secondo ordine, che quindi possa lecitamente essere approssimata in una prima
battuta o meno, non è semplicissimo da definire, perché spesso anche approcci sperimentali
che cerchino di definire l'intensità delle relazioni causali si basano su tutto un apparato
teorico che a sua volta soffre del problema di dover decidere che cosa è fondamentale e cosa
no e che quasi sempre fa uso di proxy nel momento in cui vengono svolte le misure,
quindi introducendo un grado di arbitrarietà ed imprecisione al sistema che potenzialmente
può vanificare il tentativo di misura. Non penso che una tale rivoluzione epistemica
che riconosca essenzialmente la soggettività del ricercatore, quindi la contestualità dei risultati
della ricerca, sia a priori da rifiutare in quanto irrealistica, anche perché seguendo un approccio
di complessità con metatheoria, il ricercatore esplicita quanto più possibile il contesto,
le proprie ipotesi e quindi in qualche modo diventa chiaro quali sono le premesse da cui
il risultato della ricerca segue, rendendo anche facile confutare o comunque ricondurre al corretto
ambito di applicazione pratica il risultato ottenuto. Dall'altra parte, potrebbe verosimilmente
portare a un rallentamento dell'attività di ricerca, un'attività di ricerca più curata
e più distante da dei principi di catena di montaggio e di produzione continua,
auspicio che però è comune in tante riflessioni sul futuro dell'università. Credo anche però che
questa roba sia meno vera nella pratica di quanto possa sembrare la teoria, nel senso che ci sono
domande di ricerca che nella prima forma o in variazioni che richiedono sì di rivedere ma non
di rimettere totalmente in discussione le ipotesi di lavoro, possono coprire l'intera carriera
accademica di un ricercatore o di un gruppo di ricerca, quindi con la sensibilità di periodicamente
verificare che il lavoro di ricerca non si sia distanziato troppo dalle proprie ipotesi di lavoro.
È anche facile che per alcuni ricercatori il processo di analisi, ovvero di definizione dell'oggetto
di ricerca, delle scale, dei livelli su di essi interessanti e quindi delle relative
approssimazioni e di ipotesi di lavoro conseguenti sia un lavoro che factualmente possa essere fatto
un numero limitato di volte nella vita accademica di un ricercatore a meno di ricercatori che non
come realtà poi è quello che possiamo per lo più osservare negli sviluppi dell'economia
della complessità degli ultimi anni segnare sul terzo archetipo che si chiama economia della
complessità. Ricercatori che cambino frequentemente metodi, temi, argomenti di ricerca e quindi
ripetendo questo processo di definizione della cornice di ricerca più volte nel corso della
propria carriera. Sicuramente più reali sono i limiti che l'aziendalizzazione dell'accademia
C'è da dire che essi non sono limiti intrinsechi alla produzione di conoscenza, ma norme sociali
e in ultima analisi scelte politiche della comunità in senso ampio in cui ricercatori vivono.
In questo senso abbiamo già evidenziato che due punti, che è un approccio di complessità metà
teorica solleva, che sono difficilmente conciliabili con le norme sociali e valutative in atto,
sono appunto la velocità di produzione della ricerca, per cui un approccio riduzionista
da aggiungere di variazioni sul tema e in grado di garantire una produzione maggiore di prodotti
della ricerca rispetto a un lavoro analitico a sottrarre come quello che sto proponendo.
E dall'altra parte l'indubbio vantaggio a livello di valutazione del proprio operato che hanno i
ricercatori che partecipano in comunità di ricerca grosse, con una forte identità e che
producono lavori che possano essere riconosciuti simili da altri ricercatori.
E questa necessità di accumulare citazioni, quindi riconoscimento, all'interno di una
specifica nicchia, quanto più grande meglio è della disciplina, che fa sì che tutta una serie
di ipotesi di lavoro siano necessarie da assumere per garantirsi un proseguimento di carriera,
e non possono essere di fatto scartate nemmeno se in qualche modo ostacolano lo studio del
fenomeno in oggetto, perché porterebbero il prodotto della ricerca e il lavoro del ricercatore
a distanziarsi da una comunità e quindi a distanziarsi da coloro che possono riconoscere
questo articolo non solo come valido e interessante, ma come rilevante per il loro lavoro e quindi
citarlo e quindi di fatto ritribuire il lavoro dell'autore garantendo una rendita che poi si
realizza generalmente nell'ottenimento di promozioni, di maggiori fondi o della possibilità
di allargare il proprio gruppo di ricerca. E questa intrinse camofilia nell'accademia contemporanea
è probabilmente ciò che ci impedisce di saltare gli steccati disciplinari a livello metodologico
o di conoscenza accumulata e cui attingere, di muoverci negli interstizi tra differenti tradizioni
di studio dell'economia e quindi superando una visione di pluralismo in cui ogni tradizione
deve essere opposta e mirare a superare l'altra invece che contestualizzata in ciò che si è
dimostrata capace di studiare. E alla fine è un comportamento assimilante per cui piuttosto
che mettere in discussione le pratiche di ricerca per provare a studiare in maniera più appropriata
un fenomeno nuovo si preferisce studiare un po' peggio ma con la garanzia di essere riconosciuti
parte di una comunità. In altre parole non credo ci siano limiti epistemici o di effettivo svolgimento
del lavoro di ricerca all'assumere un approccio complesso in senso metateorico in economia o in qualunque
altra disciplina. Credo però che le attuali condizioni sociali e di riproduzione dell'accademia
che vengano in maniera essenziale la possibilità di utilizzare tale approccio che non fa altro che
mettere a fuoco da una parte l'esplicitazione di dei processi impliciti che vengono già fatti
nel momento in cui si assumono certe ipotesi di lavoro piuttosto che altre spesso senza giustificarle
rispetto al preciso oggetto di studi. E dall'altra un'intuizione che mi sembra banale
che quella di dover piegare il proprio approccio e le proprie premesse all'oggetto di studi
e non l'oggetto di studi a un approccio a una premessa decisa a priori.

stilemi, globalizzazionbe della ricerca

\printbibliography

	
\end{document}